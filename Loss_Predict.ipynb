{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will take in a dataset and then predict the loss of the other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Ratio is predicted as \n",
    "\n",
    "Loss Ratio=bytes_sent/bytes_retrans​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "I need to normalize the data first, and the continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colunms without any variation:  ['wscale', 'mss', 'pmtu', 'rcvmss', 'advmss', 'rcv_space', 'rcv_ssthresh', 'ssthresh']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset manually using NumPy\n",
    "file_path = \"reno1.log.csv\"  # Update with actual file path\n",
    "# Load the dataset using numpy (semicolon separated)\n",
    "df = pd.read_csv(file_path, delimiter=\";\")  # skip_header=1 if there’s a header row\n",
    "\n",
    "# remove last column, some kind of spacing i\n",
    "# Drop the last column using pandas\n",
    "df = df.iloc[:, :-1]  # Select all rows, and all columns except the last one\n",
    "\n",
    "df['loss_ratio'] = df['bytes_retrans'] / df['bytes_sent']\n",
    "# Identify constant columns\n",
    "constant_columns = [col for col in df.columns if len(df[col].unique()) == 1]\n",
    "\n",
    "constant_columns.append(\"ssthresh\")\n",
    "print(\"colunms without any variation: \", constant_columns)\n",
    "# Drop constant columns from the dataset\n",
    "df = df.drop(columns=constant_columns)\n",
    "\n",
    "# Normalize using Min-Max scaling: (X - min) / (max - min)\n",
    "# data_min = df.min(axis=0)\n",
    "# data_max = df.max(axis=0)\n",
    "# df_normalized = (df - data_min) / (data_max - data_min)\n",
    "\n",
    "# # Now df_normalized is your dataset without the constant columns and normalized\n",
    "# print(df_normalized.iloc[0])\n",
    "# print(df_normalized.iloc[3000])\n",
    "\n",
    "# # Normalize using Min-Max scaling: (X - min) / (max - min)\n",
    "# data_min = np.min(df, axis=0)  # Find min along each column\n",
    "# data_max = np.max(df, axis=0)  # Find max along each column\n",
    "# data_normalized = (df - data_min) / (data_max - data_min)\n",
    "\n",
    "\n",
    "loss_ratio_tensor = torch.tensor(df['loss_ratio'].values, dtype=torch.float32)\n",
    "# Convert to PyTorch tensor\n",
    "#Changed df_normalized back to df first.\n",
    "data_tensor = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Ratio tensor:  torch.Size([3600])\n",
      "Data tensor:  torch.Size([3600, 12])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss Ratio tensor: \", loss_ratio_tensor.shape)\n",
    "print(\"Data tensor: \", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data, 80/10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2816, 12]) torch.Size([720, 12])\n",
      "torch.Size([2816]) torch.Size([720])\n",
      "Inputs: torch.Size([64, 12]), Targets: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(data_tensor) *0.8)\n",
    "X_train, X_test = data_tensor[64:train_size], data_tensor[train_size:]\n",
    "y_train, y_test = loss_ratio_tensor[64:train_size], loss_ratio_tensor[train_size:]\n",
    "\n",
    "# Print the shapes to verify the split\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example of accessing a batch of data\n",
    "for inputs, targets in train_loader:\n",
    "    print(f'Inputs: {inputs.shape}, Targets: {targets.shape}')\n",
    "    \n",
    "    break  # Only print the first batch for verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###LSTM Model used as the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_pt(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_pt, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers = self.num_layers, batch_first = True)\n",
    "        \n",
    "        # Linear layer for final prediction\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs, cell_state=None, hidden_state=None):\n",
    "        # Forward pass through the LSTM cell\n",
    "        if hidden_state is None or cell_state is None:\n",
    "            hidden_state = torch.zeros(1, inputs.size(0), 20).to(device)\n",
    "            cell_state = torch.zeros(1, inputs.size(0), 20).to(device)\n",
    "        hidden = (cell_state, hidden_state)\n",
    "        output, new_memory = self.lstm(inputs, hidden)\n",
    "        cell_state, hidden_state = new_memory\n",
    "        output = self.linear(output[:, -1, :])  # Take only the last time step\n",
    "        return output, cell_state, hidden_state, # Return correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will implement sliding window next time, but for now, it will only predict the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is:  torch.Size([64, 12])\n",
      "input is:  torch.Size([64, 1, 12])\n",
      "inputs.size(0) 64\n"
     ]
    }
   ],
   "source": [
    "for inputs,target in train_loader:\n",
    "    print(\"input is: \", inputs.shape)\n",
    "    inputs = inputs.unsqueeze(1).to(device)  # Reshape inputs\n",
    "    print(\"input is: \", inputs.shape)\n",
    "\n",
    "    print(\"inputs.size(0)\", inputs.size(0))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader,num_layers, hidden_size, num_epochs, learning_rate, device, ):\n",
    "    # Set the loss function and optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss=0\n",
    "        hidden_state, cell_state = None, None   \n",
    "        for inputs, targets in dataloader: \n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.unsqueeze(1).to(device)  # Reshape inputs\n",
    "            targets = targets.unsqueeze(1).to(device)\n",
    "            # Initialize hidden state and cell state for each batch\n",
    "\n",
    "            if hidden_state is not None:\n",
    "                hidden_state = hidden_state.detach()\n",
    "            if cell_state is not None:\n",
    "                cell_state = cell_state.detach()\n",
    "\n",
    "            # Forward pass\n",
    "            output, cell_state, hidden_state = model(inputs.to(device),cell_state, hidden_state)\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.00022147124754729413\n",
      "Epoch 2/100, Loss: 6.841108192258037e-07\n",
      "Epoch 3/100, Loss: 7.53638416481098e-07\n",
      "Epoch 4/100, Loss: 7.526689598045457e-07\n",
      "Epoch 5/100, Loss: 8.436082110854024e-07\n",
      "Epoch 6/100, Loss: 6.573143445300709e-07\n",
      "Epoch 7/100, Loss: 7.357368296015662e-07\n",
      "Epoch 8/100, Loss: 4.775040757142402e-07\n",
      "Epoch 9/100, Loss: 5.437888997276859e-07\n",
      "Epoch 10/100, Loss: 5.372907667061161e-07\n",
      "Epoch 11/100, Loss: 3.26775928175235e-07\n",
      "Epoch 12/100, Loss: 3.6976417862953004e-07\n",
      "Epoch 13/100, Loss: 3.210062055361815e-07\n",
      "Epoch 14/100, Loss: 2.3862338061064747e-07\n",
      "Epoch 15/100, Loss: 1.7493745425993398e-07\n",
      "Epoch 16/100, Loss: 1.6015994663831634e-07\n",
      "Epoch 17/100, Loss: 1.30476909438409e-07\n",
      "Epoch 18/100, Loss: 1.2154495139653096e-07\n",
      "Epoch 19/100, Loss: 1.0090691482389502e-07\n",
      "Epoch 20/100, Loss: 8.022148424334898e-08\n",
      "Epoch 21/100, Loss: 1.2467733270036455e-07\n",
      "Epoch 22/100, Loss: 6.986758210364935e-08\n",
      "Epoch 23/100, Loss: 5.947718455119726e-08\n",
      "Epoch 24/100, Loss: 7.464730562971185e-08\n",
      "Epoch 25/100, Loss: 8.503730390559254e-08\n",
      "Epoch 26/100, Loss: 7.852240207545885e-08\n",
      "Epoch 27/100, Loss: 7.29010253376249e-08\n",
      "Epoch 28/100, Loss: 6.158438171543036e-08\n",
      "Epoch 29/100, Loss: 6.6933827369553e-08\n",
      "Epoch 30/100, Loss: 9.877213705519497e-08\n",
      "Epoch 31/100, Loss: 8.238260958263895e-08\n",
      "Epoch 32/100, Loss: 8.893673522888693e-08\n",
      "Epoch 33/100, Loss: 9.288168214347727e-08\n",
      "Epoch 34/100, Loss: 8.346044913762088e-08\n",
      "Epoch 35/100, Loss: 1.3541365241242043e-07\n",
      "Epoch 36/100, Loss: 1.345549420570726e-07\n",
      "Epoch 37/100, Loss: 8.811955512752237e-08\n",
      "Epoch 38/100, Loss: 1.0178005743261503e-07\n",
      "Epoch 39/100, Loss: 2.3731245259803949e-07\n",
      "Epoch 40/100, Loss: 4.012253904470888e-07\n",
      "Epoch 41/100, Loss: 5.977093217136474e-07\n",
      "Epoch 42/100, Loss: 1.6901585876315617e-07\n",
      "Epoch 43/100, Loss: 1.1677632332633565e-06\n",
      "Epoch 44/100, Loss: 3.3409043026208254e-06\n",
      "Epoch 45/100, Loss: 6.759238913225183e-07\n",
      "Epoch 46/100, Loss: 4.374408432897522e-06\n",
      "Epoch 47/100, Loss: 9.607471026417225e-07\n",
      "Epoch 48/100, Loss: 4.220940472133737e-06\n",
      "Epoch 49/100, Loss: 4.2455774831105164e-07\n",
      "Epoch 50/100, Loss: 2.938919406018799e-06\n",
      "Epoch 51/100, Loss: 6.715036052863287e-07\n",
      "Epoch 52/100, Loss: 1.827194995065249e-06\n",
      "Epoch 53/100, Loss: 5.491585090908948e-06\n",
      "Epoch 54/100, Loss: 3.94118882313615e-06\n",
      "Epoch 55/100, Loss: 1.2475664010545209e-06\n",
      "Epoch 56/100, Loss: 5.926782410302746e-07\n",
      "Epoch 57/100, Loss: 1.1392003703198321e-06\n",
      "Epoch 58/100, Loss: 1.0414630460657421e-06\n",
      "Epoch 59/100, Loss: 5.003770550389546e-06\n",
      "Epoch 60/100, Loss: 5.873805770339015e-06\n",
      "Epoch 61/100, Loss: 4.757791840423212e-06\n",
      "Epoch 62/100, Loss: 1.7162165620764436e-06\n",
      "Epoch 63/100, Loss: 1.1689197576749486e-06\n",
      "Epoch 64/100, Loss: 7.578688576905599e-07\n",
      "Epoch 65/100, Loss: 7.782263032784055e-07\n",
      "Epoch 66/100, Loss: 2.648265309672256e-07\n",
      "Epoch 67/100, Loss: 1.265690914903866e-06\n",
      "Epoch 68/100, Loss: 1.0906826953513162e-06\n",
      "Epoch 69/100, Loss: 3.654404491661787e-06\n",
      "Epoch 70/100, Loss: 2.132871509145802e-06\n",
      "Epoch 71/100, Loss: 1.0587398147682785e-06\n",
      "Epoch 72/100, Loss: 2.93479364753647e-06\n",
      "Epoch 73/100, Loss: 6.163990719863157e-07\n",
      "Epoch 74/100, Loss: 1.5529768758232814e-06\n",
      "Epoch 75/100, Loss: 1.266767536685491e-06\n",
      "Epoch 76/100, Loss: 1.6778428805511423e-06\n",
      "Epoch 77/100, Loss: 2.1765669808715415e-06\n",
      "Epoch 78/100, Loss: 5.850538540983698e-06\n",
      "Epoch 79/100, Loss: 2.2922003587343522e-06\n",
      "Epoch 80/100, Loss: 5.084598110388134e-06\n",
      "Epoch 81/100, Loss: 1.6517311923859201e-06\n",
      "Epoch 82/100, Loss: 1.2812190150025449e-06\n",
      "Epoch 83/100, Loss: 1.4935733801250835e-06\n",
      "Epoch 84/100, Loss: 2.0453755967315086e-06\n",
      "Epoch 85/100, Loss: 1.0267530666493743e-06\n",
      "Epoch 86/100, Loss: 1.7230932610039091e-06\n",
      "Epoch 87/100, Loss: 1.0057483070207758e-06\n",
      "Epoch 88/100, Loss: 4.573159127991227e-06\n",
      "Epoch 89/100, Loss: 3.1402069761457794e-06\n",
      "Epoch 90/100, Loss: 1.982409145849896e-06\n",
      "Epoch 91/100, Loss: 1.5880202793154427e-07\n",
      "Epoch 92/100, Loss: 8.297731534606162e-07\n",
      "Epoch 93/100, Loss: 3.5195864746287695e-06\n",
      "Epoch 94/100, Loss: 2.1963141505891444e-06\n",
      "Epoch 95/100, Loss: 8.488191382095455e-07\n",
      "Epoch 96/100, Loss: 1.273736114400363e-06\n",
      "Epoch 97/100, Loss: 6.31451085860006e-06\n",
      "Epoch 98/100, Loss: 5.154070128909893e-06\n",
      "Epoch 99/100, Loss: 4.471001745155366e-06\n",
      "Epoch 100/100, Loss: 1.4456814817588215e-06\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "# Following the research paper's instructions\n",
    "input_size = 12\n",
    "hidden_size = 20\n",
    "num_layers = 1 # Can be changed to stack multiple LSTM layers!\n",
    "output_size = 1\n",
    "dataloader = train_loader\n",
    "#Create the model\n",
    "model = LSTM_pt(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "train(model, dataloader,num_layers, hidden_size, num_epochs = 100, learning_rate = 0.01, device = device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
